# -*- coding: utf-8 -*-
"""preprocess+models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W6Fb0FhejEXfjEz8xM3XWUqt-zrQfY2i
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from imblearn.over_sampling import SMOTE

full_df = pd.read_excel("dataset.xlsx")
full_df.head()

cols_to_drop = ['ID' , "Name"]
df = full_df.drop(cols_to_drop , axis = 1)
df.head()

gender_mapper = {"F" : 0 , "M" : 1}
df['Gender'] = df['Gender'].map(gender_mapper)
df.head()

df['Plan Type'].value_counts()

plan_mapper = {"Basic" : 1 , "Standard" : 2 , "Premium" : 3}
df['Plan Type'] = df["Plan Type"].map(plan_mapper)

df.head()

married_mapper = {"Single" : 0 , "Married" : 1}
df['Marital Status'] = df['Marital Status'].map(married_mapper)
df.head()

def good_credit(credit):
    return int(credit > 640)
df['Credit Score'] = df['Credit Score'].apply(good_credit)
df.head()

df['Policy Start Date'] = pd.to_datetime(df['Policy Start Date'] , dayfirst=True)
df.info()

df.head()

df['days_passed'] = (pd.Timestamp.today() - df['Policy Start Date']).dt.days
df.head()

future_loans_index = (df.loc[df['days_passed'] <= 0]).index

#dropping the cols with future references
df.drop(future_loans_index , inplace = True)
df.info()

churn_map = {"No" : 0 , "Yes" : 1}
df['Churn'] = df['Churn'].map(churn_map)
df.head()

int_df = df.drop("Insurance Type" , axis = 1)
str_df = df['Insurance Type']

encoded_str_df = pd.get_dummies(str_df).astype('int64')
encoded_str_df

df = pd.concat([int_df , encoded_str_df] , axis = 1)

df.head()

df.drop(['Policy Start Date'] , axis = 1 , inplace = True)
df.head()

df.head()

df.info()

plt.figure(figsize=(15, 10))
for i, col in enumerate(['Age', 'Earnings ($)', 'Claim Amount ($)', 'Insurance Plan Amount ($)', 'Credit Score', 'days_passed']):
    plt.subplot(2, 3, i+1)
    sns.histplot(df[col], kde=True)
    plt.title(f'Distribution of {col}')
plt.tight_layout()
plt.show()

plt.figure(figsize=(15, 10))
for i, col in enumerate(['Gender', 'Plan Type', 'Marital Status', 'Automobile Insurance', 'Health Insurance', 'Life Insurance']):
    plt.subplot(2, 3, i+1)
    sns.countplot(x=col, data=df, hue='Churn')
    plt.title(f'{col} vs Churn')
    plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

plt.figure(figsize=(12, 10))
corr = df.corr()
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix')
plt.show()

plt.figure(figsize=(15, 12))
for i, col in enumerate(['Age', 'Earnings ($)', 'Claim Amount ($)', 'Insurance Plan Amount ($)', 'Credit Score', 'days_passed']):
    plt.subplot(2, 3, i+1)
    sns.boxplot(x='Churn', y=col, data=df)
    plt.title(f'{col} by Churn Status')
plt.tight_layout()
plt.show()

def count_outliers(df, column):
    # Calculate Q1, Q3, and IQR
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1

    # Define the outlier boundaries
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Identify outliers
    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]

    # Count outliers
    n_outliers = len(outliers)

    return n_outliers

def drop_outliers(df, column):
    """
    Returns:
    pandas.DataFrame: DataFrame with outliers removed
    """
    # Create a copy of the DataFrame to avoid modifying the original
    df_clean = df.copy()

    # Calculate Q1, Q3, and IQR
    Q1 = df_clean[column].quantile(0.25)
    Q3 = df_clean[column].quantile(0.75)
    IQR = Q3 - Q1

    # Define the outlier boundaries
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Filter the DataFrame to remove outliers
    df_clean = df_clean[(df_clean[column] >= lower_bound) & (df_clean[column] <= upper_bound)]

    # Print information about the outliers removed
    n_outliers = len(df) - len(df_clean)
    percentage = (n_outliers / len(df)) * 100
    print(f"Removed {n_outliers} outliers ({percentage:.2f}%) from column '{column}'")
    print(f"Lower bound: {lower_bound}, Upper bound: {upper_bound}")

    return df_clean

df.info()

df = drop_outliers(df , "Claim Amount ($)")

df.info()

df['Plan Type'].value_counts()

# Print original distribution
print("Original Plan Type distribution:")
print(df['Plan Type'].value_counts())

# Separate features and target
X = df.drop('Plan Type', axis=1)  # Features (all columns except Plan Type)
y = df['Plan Type']               # Target (Plan Type)

# Initialize SMOTE
smote = SMOTE(random_state=42)

# Apply SMOTE to balance the dataset
X_resampled, y_resampled = smote.fit_resample(X, y)

# Create a new balanced dataframe
df_balanced = pd.DataFrame(X_resampled, columns=X.columns)
df_balanced['Plan Type'] = y_resampled

# Check the new distribution
print("\nBalanced Plan Type distribution after SMOTE:")
print(df_balanced['Plan Type'].value_counts())

# Keep using the balanced dataframe for further analysis
df = df_balanced

# Verify the shape of the new dataframe
print(f"\nNew dataframe shape: {df.shape}")

df["Churn"].value_counts()

# Print original distribution
print("Original Churn distribution:")
print(df['Churn'].value_counts())

# Separate features and target
X = df.drop('Churn', axis=1)  # Features (all columns except Churn)
y = df['Churn']               # Target (Churn)

# Initialize SMOTE
smote = SMOTE(random_state=42)

# Apply SMOTE to balance the dataset
X_resampled, y_resampled = smote.fit_resample(X, y)

# Create a new balanced dataframe
df_balanced = pd.DataFrame(X_resampled, columns=X.columns)
df_balanced['Churn'] = y_resampled

# Check the new distribution
print("\nBalanced Churn distribution after SMOTE:")
print(df_balanced['Churn'].value_counts())

# Keep using the balanced dataframe for further analysis
df = df_balanced

# Verify the shape of the new dataframe
print(f"\nNew dataframe shape: {df.shape}")

df.info()

df.head()

# df.to_csv("preprocessed.csv" , index_label = False)

"""Churn Predictor"""

y = df['Churn']
X = df.drop("Churn" , axis = 1)

cols_to_scale = ['Age' , 'Earnings ($)' , 'Claim Amount ($)' , 'Insurance Plan Amount ($)' ]

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train[cols_to_scale] = scaler.fit_transform(X_train[cols_to_scale])
X_test[cols_to_scale] = scaler.transform(X_test[cols_to_scale])

from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report
model = LinearSVC(dual = 'auto')
model.fit(X_train , y_train)
preds = model.predict(X_test)
print(classification_report(y_test , preds))

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)
preds = model.predict(X_test)
print(classification_report(y_test, preds))

"""Plan type recommender for churning custormers"""

df = pd.read_csv("preprocessed.csv")
df.head()

churn_df = df[df['Churn'] == 1].copy()

churn_df.head()

churn_df.drop("Churn" , axis = 1 , inplace = True)

churn_df.head()

churn_df2 = churn_df.copy()

churn_df2[cols_to_scale] = scaler.transform(churn_df2[cols_to_scale])

churn_df2.head()

churn_df2["Predicted_Churn"] = model.predict(churn_df2)
churn_df2.head()

churn_df.head()

churn_df['Plan Type'].value_counts()

X = churn_df.drop("Plan Type"  , axis = 1)
y = churn_df["Plan Type"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)

scaler2 = StandardScaler()
X_train[cols_to_scale] = scaler2.fit_transform(X_train[cols_to_scale])
X_test[cols_to_scale] = scaler2.transform(X_test[cols_to_scale])

from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report
model2 = LinearSVC(dual = 'auto')
model2.fit(X_train , y_train)
preds2 = model2.predict(X_test)
print(classification_report(y_test , preds2))

"""Plan type recommender for non churning customers"""

df = pd.read_csv("preprocessed.csv")
df.head()

non_churn_df = df[df['Churn'] == 0].copy()

non_churn_df.head()

non_churn_df.drop("Churn" , axis = 1 , inplace = True)

non_churn_df.head()

non_churn_df2 = non_churn_df.copy()

non_churn_df2[cols_to_scale] = scaler.transform(non_churn_df2[cols_to_scale])

non_churn_df2.head()

non_churn_df2["Predicted_Churn"] = model.predict(non_churn_df2)
non_churn_df2.head()

non_churn_df.head()

non_churn_df['Plan Type'].value_counts()

X = non_churn_df.drop("Plan Type"  , axis = 1)
y = non_churn_df["Plan Type"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)

scaler3 = StandardScaler()
X_train[cols_to_scale] = scaler3.fit_transform(X_train[cols_to_scale])
X_test[cols_to_scale] = scaler3.transform(X_test[cols_to_scale])

from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report
model3 = LinearSVC(dual = 'auto')
model3.fit(X_train , y_train)
preds3 = model3.predict(X_test)
print(classification_report(y_test , preds3))

"""Saving models and scalers

inputs are age , gender , earnings , claim amount , insurance plan amount , credit score (binary col) , marital status , days_passed and type of insurance One hot encoded

cols_to_scale = [Age , Earnings ($) , Claim Amount ($) , Insurance Plan Amount ($) ]
"""

import joblib

"""For churn models"""

joblib.dump(model , "churn_model.pkl")
joblib.dump(scaler , "churn_scaler.pkl")

"""For plan type recommender for churning customers"""

joblib.dump(model2 , "churning_plan_recommender.pkl")
joblib.dump(scaler2 , "churning_plan_recommender_scaler.pkl")

joblib.dump(model3 , "non_churning_plan_recommender.pkl")
joblib.dump(scaler3 , "non_churning_plan_recommender_scaler.pkl")

